{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee44ab3",
   "metadata": {},
   "source": [
    "## Extension: Grid World\n",
    "\n",
    "Consider a robot navigating in a grid-based environment. Each cell in the grid represents a distinct state of the surroundings. The robot can take four deterministic actions at each cell: \"up,\" \"down,\" \"left,\" and \"right,\" resulting in the robot to move precisely one cell in the corresponding direction on the grid. Actions that would take the agent off the grid are not allowed. Within the grid, certain states (orange) correspond to undesirable conditions, such as rough terrain, while one state (green) represents the ultimate goal.\n",
    "\n",
    "Upon reaching the goal state, the robot gains a reward of 1. Conversely, traversing the rough terrain incurs a penalty (or negative reward) of 10. Additionally, every move the robot makes entails a penalty of 1. The robot's primary objective is to efficiently reach the goal state, aiming to maximize the total reward (minimize the total penalty) incurred. This entails both avoiding the rough terrain and efficiently navigating through the grid.\n",
    "\n",
    "<img src=\"grid_world.png\" alt=\"Image\" width=\"300\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2285a969-d9b8-465b-9041-a280de02647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Try to utilize dynamic programming algorithm to address the grid world problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3435a1f",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "To solve the grid world problem using dynamic programming, we can apply the **Value Iteration** algorithm. This method iteratively updates the value function for each state based on the expected rewards of possible actions.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Initialize Values:** Assign an initial value of zero to the value function of all states.\n",
    "\n",
    "2. **Update Values:**  Apply Bellman's equation to update the value function:\n",
    "\n",
    "$ V(s) \\leftarrow \\max_{a} \\left[ \\text{Reward}(s, a) + \\gamma \\cdot \\sum_{s'} \\text{Transition Probability}(s, a, s') \\cdot V(s') \\right]$\n",
    "\n",
    "   Where:\n",
    "   *  `V(s)` is the value of state `s`\n",
    "   *  `maxₐ` denotes the maximum value over all possible actions `a`\n",
    "   *  `Reward(s, a)` is the reward obtained by taking action `a` in state `s`\n",
    "   *  `γ` is the discount factor\n",
    "   *  `Σₛ'` represents the sum over all possible next states `s'`\n",
    "   *  `Transition Probability(s, a, s')` is the probability of transitioning to state `s'` from state `s` when taking action `a`\n",
    "\n",
    "3. **Convergence:** Repeat step 2 until the values converge. Convergence is achieved when the change in the value function between iterations is less than a predefined threshold (`θ`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c925b",
   "metadata": {},
   "source": [
    "### Q2\n",
    "What challenge or issue are you currently facing? Provide an explanation for the nature of this problem. In Practical 4, we will explore strategies to address this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2bcaa",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "**1. Speed of Convergence:**\n",
    "\n",
    "* **Challenge:** Value iteration can converge slowly, especially in large or complex networks.\n",
    "* **Explanation:** Convergence speed depends on the number of iterations needed to reach the threshold. Larger networks or those with intricate reward structures may require more iterations, leading to slower convergence.\n",
    "\n",
    "**2. Handling Large Networks:**\n",
    "\n",
    "* **Challenge:** Computational resources and time requirements increase significantly with network size.\n",
    "* **Explanation:** As the network expands, the number of states and actions grows, demanding more computational power. This can result in higher memory usage and extended processing times.\n",
    "\n",
    "**3. Accuracy and Threshold Management:**\n",
    "\n",
    "* **Challenge:** Selecting the right convergence threshold (`θ`) is crucial for balancing computational efficiency and solution accuracy.\n",
    "* **Explanation:** A high threshold might cause premature convergence, yielding a suboptimal solution. Conversely, a low threshold could demand excessive computation without guaranteeing optimality. Finding the optimal threshold is essential for efficient value iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7079286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid world as a matrix using np.array. Each entry correspond to its reward.\n",
    "grid = np.array([\n",
    "    [0, 0, -10, 0],\n",
    "    [0, 0, 0, 0],\n",
    "    [0, 0, -10, 0],\n",
    "    [0, 0, -10, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6943fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the value function as a zero matrix with the same shape with the grid.\n",
    "values = np.zeros_like(grid, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ca30b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get next state. The action includes \"up\", \"down\", \"left\", \"right\".\n",
    "def get_next_state(i, j, action):\n",
    "  if action == \"up\":\n",
    "      i -= 1\n",
    "  elif action == \"down\":\n",
    "      i += 1\n",
    "  elif action == \"left\":\n",
    "      j -= 1\n",
    "  elif action == \"right\":\n",
    "      j += 1\n",
    "  return i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "997ea551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to check if the next state is valid. The states beyond the grid are not valid. This function returns Boolean value.\n",
    "def is_valid_state(i, j, grid):\n",
    "  rows, cols = grid.shape\n",
    "  return 0 <= i < rows and 0 <= j < cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01J4SZ3T4ZTEDQ7V6JNZFE3Y26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Values:\n",
      " [[3.07063342 3.41967008 3.80670307 4.23603276]\n",
      " [3.41967008 3.80670307 4.23603276 4.71242949]\n",
      " [3.07770307 3.42603276 4.71242949 5.24118654]\n",
      " [2.76993276 3.08342949 5.24118654 4.71706788]]\n"
     ]
    }
   ],
   "source": [
    "# Value Iteration Algorithm\n",
    "theta = 0.01  # Convergence threshold\n",
    "gamma = 0.9   # Discount factor\n",
    "\n",
    "while True:\n",
    "    delta = 0\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[1]):\n",
    "            v = values[i, j]\n",
    "            max_value = float('-inf')\n",
    "            for action in [\"up\", \"down\", \"left\", \"right\"]:\n",
    "                next_i, next_j = get_next_state(i, j, action)\n",
    "                if is_valid_state(next_i, next_j, grid):\n",
    "                    next_value = grid[next_i, next_j] + gamma * values[next_i, next_j]\n",
    "                    max_value = max(max_value, next_value)\n",
    "            values[i, j] = max_value\n",
    "            delta = max(delta, abs(v - values[i, j]))\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "print(\"Optimal Values:\\n\", values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
